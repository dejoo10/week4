{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks with Scikit-Learn\n",
        "\n",
        "> for more info [here](https://stackabuse.com/introduction-to-neural-networks-with-scikit-learn/)"
      ],
      "metadata": {
        "id": "BGVymvSbW-TM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsjeb88Fj5og"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Location of dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "\n",
        "# Assign column names to the dataset\n",
        "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
        "\n",
        "# Read dataset to pandas dataframe\n",
        "irisdata = pd.read_csv(url, names=names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "irisdata.head()"
      ],
      "metadata": {
        "id": "6rgZPNfakEiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign data from first four columns to X variable\n",
        "X = irisdata.iloc[:, 0:4]\n",
        "\n",
        "# Assign data from fifth column to y variable\n",
        "y = irisdata.iloc[:, 4]\n"
      ],
      "metadata": {
        "id": "tZ8QfvB_kIV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, iloc[:, 4] selects all rows from the fifth column of irisdata and assigns it to the variable y.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qcj-Upy1kl2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "vTrtbw-mkOLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's see how many unique values we have in our y series."
      ],
      "metadata": {
        "id": "OIQLskV0lO8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.unique()"
      ],
      "metadata": {
        "id": "7ZnQ_5uskwTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have three unique classes: 'Iris-setosa', 'Iris-versicolor' and 'Iris-virginica'. Let's convert these categorical values to numerical values. To do so we will use Scikit-Learn's LabelEncoder class."
      ],
      "metadata": {
        "id": "OX0B_SOalSaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "# Fit and transform y\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ga8aycbWk3hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " if you check unique values in the y_encoded series, you will see following results:"
      ],
      "metadata": {
        "id": "FUBiWX1nrJgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Convert y_encoded back to pandas Series\n",
        "y_series = pd.Series(y_encoded)\n",
        "\n",
        "# Print the pandas Series\n",
        "print(y_series.unique())\n"
      ],
      "metadata": {
        "id": "MOJgMvfMqCbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will divide our dataset into training and test splits. The training data will be used to train the neural network and the test data will be used to evaluate the performance of the neural network."
      ],
      "metadata": {
        "id": "LvMuDwBllnZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_series, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "OArWZxt1lqmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before making actual predictions, it is always a good practice to scale the features so that all of them can be uniformly evaluated. Feature scaling is performed only on the training data and not on test data. This is because in the real world, data is not scaled and the ultimate purpose of the neural network is to make predictions on real world data. Therefore, we try to keep our test data as real as possible."
      ],
      "metadata": {
        "id": "RSSN23zArafy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "fzDHQ-jorcuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanantion\n",
        "\n",
        "In the line `scaler = StandardScaler()`, the `StandardScaler` method from the `sklearn.preprocessing` module is used to perform standardization on the features.\n",
        "\n",
        "Standardization (or Z-score normalization) scales the features such that they have a mean of 0 and a standard deviation of 1. The formula for standardization is:\n",
        "\n",
        "\\[ \\text{Standardized value} = \\frac{\\text{Original value} - \\text{Mean}}{\\text{Standard deviation}} \\]\n",
        "\n",
        "Here's a brief overview of how `StandardScaler` works:\n",
        "\n",
        "1. **Fit**: When you call `scaler.fit(X_train)`, the `StandardScaler` computes the mean and standard deviation of each feature in the training set `X_train`.\n",
        "\n",
        "2. **Transform**: After fitting the scaler, you can use it to transform the training set to standardize the features using the computed mean and standard deviation. This is done using the `scaler.transform()` method.\n",
        "\n",
        "### Here's an example:\n",
        "\n"
      ],
      "metadata": {
        "id": "uahTRgvFtYFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create the dataset\n",
        "data = np.array([[25, 50000],\n",
        "                 [30, 75000],\n",
        "                 [35, 100000],\n",
        "                 [40, 60000],\n",
        "                 [45, 90000]])\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on the entire data\n",
        "scaler.fit(data)\n",
        "\n",
        "# Transform the entire data\n",
        "data_scaled = scaler.transform(data)\n",
        "\n",
        "# Print original and scaled data\n",
        "print(\"Original Data:\")\n",
        "print(data)\n",
        "print(\"\\nScaled Data:\")\n",
        "print(data_scaled)\n"
      ],
      "metadata": {
        "id": "AMMSmhFKsYi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now it's  time to train a neural network that can actually make predictions."
      ],
      "metadata": {
        "id": "6Is_UuIlyg3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n",
        "mlp.fit(X_train, y_train.values.ravel())"
      ],
      "metadata": {
        "id": "w6oTEr9Nt0Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to import the MLPClassifier class from the sklearn.neural_network library. In the second line, this class is initialized with two parameters.\n",
        "\n",
        "The first parameter, hidden_layer_sizes, is used to set the size of the hidden layers. In our script we will create three layers of 10 nodes each. There is no standard formula for choosing the number of layers and nodes for a neural network and it varies quite a bit depending on the problem at hand. The best way is to try different combinations and see what works best.\n",
        "\n",
        "The second parameter to MLPClassifier specifies the number of iterations, or the epochs, that you want your neural network to execute. Remember, one epoch is a combination of one cycle of feed-forward and back propagation phase.\n",
        "\n",
        "By default the 'ReLU' activation function is used with adam cost optimizer. However, you can change these functions using the activation and solver parameters, respectively.\n",
        "\n",
        "In the third line the fit function is used to train the algorithm on our training data i.e. X_train and y_train."
      ],
      "metadata": {
        "id": "wpqcTWa8ysQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions\n",
        "The final step is to make predictions on our test data. To do so, execute the following script:"
      ],
      "metadata": {
        "id": "FSLKq6KHyzWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = mlp.predict(X_test)"
      ],
      "metadata": {
        "id": "nETud_taywYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Algorithm"
      ],
      "metadata": {
        "id": "CoPzhng-y4tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "print(conf_matrix)\n",
        "print(classification_report(y_test,predictions))"
      ],
      "metadata": {
        "id": "x34kQkIry9RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down each part of the output:\n",
        "\n",
        "### Confusion Matrix:\n",
        "```\n",
        "[[10  0  0]\n",
        " [ 0  9  0]\n",
        " [ 0  0 11]]\n",
        "```\n",
        "- The confusion matrix shows the counts of true positive, false positive, true negative, and false negative predictions for each class.\n",
        "- In this matrix:\n",
        "  - Class `0` has 10 true positives.\n",
        "  - Class `1` has 9 true positives.\n",
        "  - Class `2` has 11 true positives.\n",
        "  - There are no false positives or false negatives for any class.\n",
        "\n",
        "### Classification Report:\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "```\n",
        "- **Precision**: Precision is the ratio of true positive predictions to the total predicted positives. It indicates how many of the items identified as positive are actually positive.\n",
        "  \n",
        "- **Recall (Sensitivity)**: Recall is the ratio of true positive predictions to the total actual positives. It indicates how many of the actual positive items were identified correctly.\n",
        "  \n",
        "- **F1-Score**: The F1-score is the harmonic mean of precision and recall, providing a balance between the two metrics. It is a measure of a test's accuracy and is particularly useful when the classes are imbalanced.\n",
        "  \n",
        "- **Support**: Support is the number of actual occurrences of the class in the specified dataset.\n",
        "\n",
        "### Analysis:\n",
        "\n",
        "- **Accuracy**: The overall accuracy of the model is 1.00 or 100%, which means all predictions made by the model are correct for the given test dataset.\n",
        "\n",
        "- **Per-Class Metrics**:\n",
        "  - For class `0`: Precision, Recall, and F1-score are all 1.00, indicating perfect predictions for this class.\n",
        "  - For class `1`: Precision, Recall, and F1-score are all 1.00, indicating perfect predictions for this class.\n",
        "  - For class `2`: Precision, Recall, and F1-score are all 1.00, indicating perfect predictions for this class.\n"
      ],
      "metadata": {
        "id": "mN8Zl3kYzGpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Define class labels\n",
        "class_labels = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=class_labels, yticklabels=class_labels)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jPhmTsDQzIPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:\n",
        "The model appears to perform exceptionally well on the test dataset, achieving perfect precision, recall, and F1-score for each class. However, it's important to note that this high level of performance might be indicative of an overfit model, especially if the model has not been evaluated on a separate validation set.\n",
        "\n",
        "### Ref\n",
        "\n",
        "- [https://stackabuse.com/introduction-to-neural-networks-with-scikit-learn/](https://stackabuse.com/introduction-to-neural-networks-with-scikit-learn/)"
      ],
      "metadata": {
        "id": "luHY4fwQ4SM2"
      }
    }
  ]
}