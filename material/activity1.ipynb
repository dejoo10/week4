{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YIIM9aSEB9"
      },
      "source": [
        "\n",
        "## Perceptron\n",
        "\n",
        "> More info [here](https://uw-madison-datascience.github.io/2022-10-26-machine-learning-novice-sklearn/06-neural-networks/index.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KX8BuU_gJlKf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def perceptron(inputs, weights, threshold):\n",
        "\n",
        "    assert len(inputs) == len(weights)\n",
        "\n",
        "    # multiply the inputs and weights\n",
        "    values = np.multiply(inputs,weights)\n",
        "\n",
        "    # sum the results\n",
        "    total = sum(values)\n",
        "\n",
        "    # decide if we should activate the perceptron\n",
        "    if total < threshold:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg7u8-ZtSNP5"
      },
      "source": [
        "### Computing with a perceptron\n",
        "\n",
        "A single perceptron can perform basic linear classification problems such as computing the logical AND, OR, and NOT functions.\n",
        "\n",
        "OR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EKNEmB7JpJ6"
      },
      "outputs": [],
      "source": [
        "inputs = [[0.0,0.0],[1.0,0.0],[0.0,1.0],[1.0,1.0]]\n",
        "for input in inputs:\n",
        "    print(input,perceptron(input, [0.5,0.5], 0.5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLliL4_xJtrR"
      },
      "outputs": [],
      "source": [
        "inputs = [[0.0,0.0],[1.0,0.0],[0.0,1.0],[1.0,1.0]]\n",
        "for input in inputs:\n",
        "    print(input,perceptron(input, [0.5,0.5], 1.0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMfAKPF4Jv0R"
      },
      "outputs": [],
      "source": [
        "inputs = [[0.0,1.0],[1.0,1.0]]\n",
        "for input in inputs:\n",
        "    print(input,perceptron(input, [-1.0,1.0], 1.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1izw41KJS7w2"
      },
      "source": [
        "--------\n",
        "## Implementing a Multi-Layer Perceptron from Scratch\n",
        "\n",
        "> Detailed explanation [here](https://github.com/ML-2024/week4/blob/main/datasets/Build-MLP-from-Scratch.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1aivqRFRTOTj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(int)  # 1 if Iris-Virginica, else 0\n",
        "y = y.reshape([150, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D2nbG4NEUdJC"
      },
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QY_bAfHZTmWy"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2FkpB5GPTp5i"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights1 = np.random.randn(self.input_size, self.hidden_size) * 0.01\n",
        "        self.weights2 = np.random.randn(self.hidden_size, self.output_size) * 0.01\n",
        "        self.bias1 = np.zeros((1, self.hidden_size))\n",
        "        self.bias2 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def fit(self, X, y, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            # Feedforward\n",
        "            layer1 = X.dot(self.weights1) + self.bias1\n",
        "            activation1 = sigmoid(layer1)\n",
        "            layer2 = activation1.dot(self.weights2) + self.bias2\n",
        "            activation2 = sigmoid(layer2)\n",
        "\n",
        "            # Backpropagation\n",
        "            error = activation2 - y\n",
        "            d_weights2 = activation1.T.dot(error * sigmoid_derivative(layer2))\n",
        "            d_bias2 = np.sum(error * sigmoid_derivative(layer2), axis=0, keepdims=True)\n",
        "\n",
        "            error_hidden = error.dot(self.weights2.T) * sigmoid_derivative(layer1)\n",
        "            d_weights1 = X.T.dot(error_hidden)\n",
        "            d_bias1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights2 -= self.learning_rate * d_weights2\n",
        "            self.bias2 -= self.learning_rate * d_bias2\n",
        "            self.weights1 -= self.learning_rate * d_weights1\n",
        "            self.bias1 -= self.learning_rate * d_bias1\n",
        "\n",
        "    def predict(self, X):\n",
        "        layer1 = X.dot(self.weights1) + self.bias1\n",
        "        activation1 = sigmoid(layer1)\n",
        "        layer2 = activation1.dot(self.weights2) + self.bias2\n",
        "        activation2 = sigmoid(layer2)\n",
        "        return (activation2 > 0.5).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7UHcrOITrcD"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the MLP class\n",
        "mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
        "\n",
        "# Train the MLP\n",
        "mlp.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = mlp.predict(X)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = np.mean(y_pred == y)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt9SQ9seThqC"
      },
      "source": [
        "This code displays a scatter plot showing the relationships between petal length and petal width for the three Iris species."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BHE4r5STRvS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "# Scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Iris-Setosa', c='r', edgecolors='k')\n",
        "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Iris-Versicolour', c='g', edgecolors='k')\n",
        "plt.scatter(X[y == 2][:, 0], X[y == 2][:, 1], label='Iris-Virginica', c='b', edgecolors='k')\n",
        "\n",
        "# Plot formatting\n",
        "plt.title('Scatter Plot of Iris Dataset')\n",
        "plt.xlabel('Petal Length (cm)')\n",
        "plt.ylabel('Petal Width (cm)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1iOB0UR80P"
      },
      "source": [
        "---------\n",
        "## Ref\n",
        "\n",
        "- https://uw-madison-datascience.github.io/2022-10-26-machine-learning-novice-sklearn/06-neural-networks/index.html\n",
        "- https://humphryscomputing.com/Notes/Neural/single.neural.html\n",
        "- https://abtinmy.github.io/CS-SBU-NeuralNetwork/lectures/introduction/MLP-Scratch-Iris\n",
        "- https://medium.com/@hirok4/building-a-multi-layer-perceptron-from-scratch-c9679752cf48"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
